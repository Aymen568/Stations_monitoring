# Use an official Spark runtime as a parent image
FROM bitnami/spark:3.2.0
# Set the working directory to /app
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY ./pyspark_consumer.py ./

# Install any needed packages specified in requirements.txt
USER root
RUN pip install --trusted-host pypi.python.org elasticsearch

# Define environment variables for Spark
ENV SPARK_HOME /opt/bitnami/spark
ENV PYSPARK_PYTHON python3
ENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-<version>-src.zip

# Run consumer.py when the container launches
CMD ["/bin/bash", "-c", "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.elasticsearch:elasticsearch-spark-30_2.12:7.15.0 pyspark_consumer.py"]
